{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>\n",
    "    \n",
    "# **Remaking the World through Machine Learning**\n",
    "## PG Workshop\n",
    "\n",
    "[<img src=\"UJLogo.jpg\" width=\"250\"/>](UJLogo.jpg)\n",
    "</center></h1>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Naive Bayes Classifier From Scratch in Python\n",
    "#### by Jason Brownlee on October 18, 2019 in Code Algorithms From Scratch\n",
    "\n",
    "https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/\n",
    "\n",
    "Bayes’ Theorem provides a way that we can calculate the probability of a piece of data belonging to a given class, given our prior knowledge. Bayes’ Theorem is stated as:\n",
    "\n",
    "$$P(class|data) = (P(data|class) * P(class)) / P(data)$$\n",
    "\n",
    "Where P(class|data) is the probability of class given the provided data.\n",
    "\n",
    "Naive Bayes is a classification algorithm for binary (two-class) and multiclass classification problems. It is called Naive Bayes or idiot Bayes because the calculations of the probabilities for each class are simplified to make their calculations tractable.\n",
    "\n",
    "Rather than attempting to calculate the probabilities of each attribute value, they are assumed to be conditionally independent given the class value.\n",
    "\n",
    "This is a very strong assumption that is most unlikely in real data, i.e. that the attributes do not interact. Nevertheless, the approach performs surprisingly well on data where this assumption does not hold.\n",
    "\n",
    "### Iris Flower Species Dataset\n",
    "In this tutorial we will use the Iris Flower Species Dataset.\n",
    "\n",
    "The Iris Flower Dataset involves predicting the flower species given measurements of iris flowers.\n",
    "\n",
    "It is a multiclass classification problem. The number of observations for each class is balanced. There are 150 observations with 4 input variables and 1 output variable. The variable names are as follows:\n",
    "- Sepal length in cm.\n",
    "- Sepal width in cm.\n",
    "- Petal length in cm.\n",
    "- Petal width in cm.\n",
    "- Class\n",
    "\n",
    "A portion of the data set is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Predictions with Naive Bayes On The Iris Dataset\n",
    "\n",
    "from csv import reader\n",
    "from math import sqrt\n",
    "from math import exp\n",
    "from math import pi\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['4.9', '3', '1.4', '0.2', 'Iris-setosa'],\n",
       " ['4.7', '3.2', '1.3', '0.2', 'Iris-setosa'],\n",
       " ['4.6', '3.1', '1.5', '0.2', 'Iris-setosa'],\n",
       " ['5', '3.6', '1.4', '0.2', 'Iris-setosa']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See an example of what the data set looks like:\n",
    "filename = 'iris.csv'\n",
    "dataset = load_csv(filename)\n",
    "dataset[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Tutorial (in 5 easy steps)\n",
    "First we will develop each piece of the algorithm in this section, then we will tie all of the elements together into a working implementation applied to a real dataset in the next section.\n",
    "\n",
    "This Naive Bayes tutorial is broken down into 5 parts:\n",
    "- Step 1: Separate By Class.\n",
    "- Step 2: Summarize Dataset.\n",
    "- Step 3: Summarize Data By Class.\n",
    "- Step 4: Gaussian Probability Density Function.\n",
    "- Step 5: Class Probabilities.\n",
    "\n",
    "These steps will provide the foundation that you need to implement Naive Bayes from scratch and apply it to your own predictive modeling problems.\n",
    "\n",
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string column to float\n",
    "\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "        \n",
    "# Convert string column to integer\n",
    "\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "        print('[%s] => %d' % (value, i))\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Separate By Class\n",
    "Split the dataset by class values, returns a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_by_class(dataset):\n",
    "    separated = dict()\n",
    "    for i in range(len(dataset)):\n",
    "        vector = dataset[i]\n",
    "        class_value = vector[-1]\n",
    "        if (class_value not in separated):\n",
    "            separated[class_value] = list()\n",
    "        separated[class_value].append(vector)\n",
    "    return separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[3.393533211, 2.331273381, 0]\n",
      "[3.110073483, 1.781539638, 0]\n",
      "[1.343808831, 3.368360954, 0]\n",
      "[3.582294042, 4.67917911, 0]\n",
      "[2.280362439, 2.866990263, 0]\n",
      "1\n",
      "[7.423436942, 4.696522875, 1]\n",
      "[5.745051997, 3.533989803, 1]\n",
      "[9.172168622, 2.511101045, 1]\n",
      "[7.792783481, 3.424088941, 1]\n",
      "[7.939820817, 0.791637231, 1]\n"
     ]
    }
   ],
   "source": [
    "#CHECK:\n",
    "\n",
    "# Test dataset by class\n",
    "dataset = [[3.393533211,2.331273381,0],\n",
    "[3.110073483,1.781539638,0],\n",
    "[1.343808831,3.368360954,0],\n",
    "[3.582294042,4.67917911,0],\n",
    "[2.280362439,2.866990263,0],\n",
    "[7.423436942,4.696522875,1],\n",
    "[5.745051997,3.533989803,1],\n",
    "[9.172168622,2.511101045,1],\n",
    "[7.792783481,3.424088941,1],\n",
    "[7.939820817,0.791637231,1]]\n",
    "\n",
    "\n",
    "separated = separate_by_class(dataset)\n",
    "for label in separated:\n",
    "    print(label)\n",
    "    for row in separated[label]:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Summarize Dataset\n",
    "We need two statistics from a given set of data.\n",
    "\n",
    "We’ll see how these statistics are used in the calculation of probabilities in a few steps. The two statistics we require from a given dataset are the mean and the standard deviation (average deviation from the mean).\n",
    "\n",
    "The mean is the average value and can be calculated as:\n",
    "\n",
    "$$mean = sum(x)/n * count(x)$$\n",
    "\n",
    "Where x is the list of values or a column we are looking.\n",
    "\n",
    "Below is a small function named ```mean()``` that calculates the mean of a list of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of a list of numbers\n",
    "\n",
    "def mean(numbers):\n",
    "    return sum(numbers)/float(len(numbers))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample standard deviation is calculated as the mean difference from the mean value. This can be calculated as:\n",
    "\n",
    "$$\\text{standard deviation} = \\sqrt{\\sum_i^N (x_i – mean(x))^2 / N-1}$$\n",
    "You can see that we square the difference between the mean and a given value, calculate the average squared difference from the mean, then take the square root to return the units back to their original value.\n",
    "\n",
    "Below is a small function named ```standard_deviation()``` that calculates the standard deviation of a list of numbers. You will notice that it calculates the mean. It might be more efficient to calculate the mean of a list of numbers once and pass it to the ```standard_deviation()``` function as a parameter. You can explore this optimization if you’re interested later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the standard deviation of a list of numbers\n",
    "\n",
    "def stdev(numbers):\n",
    "    avg = mean(numbers)\n",
    "    variance = sum([(x-avg)**2 for x in numbers]) / float(len(numbers)-1)\n",
    "    return sqrt(variance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We require the mean and standard deviation statistics to be calculated for each input attribute or each column of our data.\n",
    "\n",
    "We can do that by gathering all of the values for each column into a list and calculating the mean and standard deviation on that list. Once calculated, we can gather the statistics together into a list or tuple of statistics. Then, repeat this operation for each column in the dataset and return a list of tuples of statistics.\n",
    "\n",
    "Below is a function named ```summarize_dataset()``` that implements this approach. It uses some Python tricks to cut down on the number of lines required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean, stdev and count for each column in a dataset\n",
    "\n",
    "def summarize_dataset(dataset):\n",
    "    summaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)]\n",
    "    del(summaries[-1])\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5.178333386499999, 2.7665845055177263, 10), (2.9984683241, 1.218556343617447, 10)]\n"
     ]
    }
   ],
   "source": [
    "#CHECK:\n",
    "    \n",
    "summary = summarize_dataset(dataset)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Summarize Data By Class\n",
    "We require statistics from our training dataset organized by class.\n",
    "\n",
    "Above, we have developed the ```separate_by_class()``` function to separate a dataset into rows by class. And we have developed ```summarize_dataset()``` function to calculate summary statistics for each column.\n",
    "\n",
    "We can put all of this together and summarize the columns in the dataset organized by class values.\n",
    "\n",
    "Below is a function named ```summarize_by_class()``` that implements this operation. The dataset is first split by class, then statistics are calculated on each subset. The results in the form of a list of tuples of statistics are then stored in a dictionary by their class value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset by class then calculate statistics for each row\n",
    "\n",
    "def summarize_by_class(dataset):\n",
    "    separated = separate_by_class(dataset)\n",
    "    summaries = dict()\n",
    "    for class_value, rows in separated.items():\n",
    "        summaries[class_value] = summarize_dataset(rows)\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(2.7420144012, 0.9265683289298018, 5)\n",
      "(3.0054686692, 1.1073295894898725, 5)\n",
      "1\n",
      "(7.6146523718, 1.2344321550313704, 5)\n",
      "(2.9914679790000003, 1.4541931384601618, 5)\n"
     ]
    }
   ],
   "source": [
    "#CHECK:\n",
    "    \n",
    "summary = summarize_by_class(dataset)\n",
    "for label in summary:\n",
    "    print(label)\n",
    "    for row in summary[label]:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Gaussian Probability Density Function\n",
    "Calculating the probability or likelihood of observing a given real-value like $X_1$ is difficult.\n",
    "\n",
    "One way we can do this is to assume that $X_1$ values are drawn from a distribution, such as a bell curve or Gaussian distribution.\n",
    "\n",
    "A Gaussian distribution can be summarized using only two numbers: the mean and the standard deviation. Therefore, with a little math, we can estimate the probability of a given value. This piece of math is called a Gaussian Probability Distribution Function (or Gaussian PDF) and can be calculated as:\n",
    "\n",
    "$$f(x) = \\frac{1}{\\sqrt{2  \\pi}  \\sigma}  exp\\left(-\\left(\\frac{(x-mean)^2}{2  \\sigma^2}\\right)\\right)$$\n",
    "where $\\sigma$ is the standard deviation for $x$ and mean is the mean for $x$.\n",
    "\n",
    "Below is a function that implements this. I tried to split it up to make it more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Gaussian probability distribution function for x\n",
    "\n",
    "def calculate_probability(x, mean, stdev):\n",
    "    exponent = exp(-((x-mean)**2 / (2 * stdev**2 )))\n",
    "    return (1 / (sqrt(2 * pi) * stdev)) * exponent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3989422804014327\n",
      "0.24197072451914337\n",
      "0.24197072451914337\n"
     ]
    }
   ],
   "source": [
    "# CHECK Gaussian PDF\n",
    "print(calculate_probability(1.0, 1.0, 1.0))\n",
    "print(calculate_probability(2.0, 1.0, 1.0))\n",
    "print(calculate_probability(0.0, 1.0, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Class Probabilities\n",
    "Now it is time to use the statistics calculated from our training data to calculate probabilities for new data.\n",
    "\n",
    "Probabilities are calculated separately for each class. This means that we first calculate the probability that a new piece of data belongs to the first class, then calculate probabilities that it belongs to the second class, and so on for all the classes.\n",
    "\n",
    "The probability that a piece of data belongs to a class is calculated as follows:\n",
    "\n",
    "$$P(class|data) = P(X|class) * P(class)$$\n",
    "\n",
    "You may note that this is different from the Bayes Theorem described above.\n",
    "\n",
    "The division has been removed to simplify the calculation.\n",
    "\n",
    "This means that the result is no longer strictly a probability of the data belonging to a class. The value is still maximized, meaning that the calculation for the class that results in the largest value is taken as the prediction. This is a common implementation simplification as we are often more interested in the class prediction rather than the probability.\n",
    "\n",
    "The input variables are treated separately, giving the technique it’s name \"naive\". For the above example where we have 2 input variables, the calculation of the probability that a row belongs to the first class 0 can be calculated as:\n",
    "\n",
    "$$P(class=0|X1,X2) = P(X1|class=0) * P(X2|class=0) * P(class=0)$$\n",
    "\n",
    "Now you can see why we need to separate the data by class value. The Gaussian Probability Density function in the previous step is how we calculate the probability of a real value like X1 and the statistics we prepared are used in this calculation.\n",
    "\n",
    "Below is a function named ```calculate_class_probabilities()``` that ties all of this together.\n",
    "\n",
    "It takes a set of prepared summaries and a new row as input arguments.\n",
    "\n",
    "First the total number of training records is calculated from the counts stored in the summary statistics. This is used in the calculation of the probability of a given class or $P(class)$ as the ratio of rows with a given class of all rows in the training data.\n",
    "\n",
    "Next, probabilities are calculated for each input value in the row using the Gaussian probability density function and the statistics for that column and of that class. Probabilities are multiplied together as they accumulated.\n",
    "\n",
    "This process is repeated for each class in the dataset.\n",
    "\n",
    "Finally a dictionary of probabilities is returned with one entry for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the probabilities of predicting each class for a given row: P(c_i/x)\n",
    "\n",
    "def calculate_class_probabilities(summaries, row):\n",
    "    total_rows = sum([summaries[label][0][2] for label in summaries])\n",
    "    probabilities = dict()\n",
    "    for class_value, class_summaries in summaries.items():\n",
    "        probabilities[class_value] = summaries[class_value][0][2]/float(total_rows)\n",
    "        for i in range(len(class_summaries)):\n",
    "            mean, stdev, _ = class_summaries[i]\n",
    "            probabilities[class_value] *= calculate_probability(row[i], mean, stdev)\n",
    "    return probabilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class for a given row: P(c_i/x)\n",
    "\n",
    "def predict(summaries, row):\n",
    "    probabilities = calculate_class_probabilities(summaries, row)\n",
    "    best_label, best_prob = None, -1\n",
    "    for class_value, probability in probabilities.items():\n",
    "        if best_label is None or probability > best_prob:\n",
    "            best_prob = probability\n",
    "            best_label = class_value\n",
    "    return best_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.05032427673372075, 1: 0.00011557718379945765}\n"
     ]
    }
   ],
   "source": [
    "#CHECK:\n",
    "    \n",
    "summaries = summarize_by_class(dataset)\n",
    "probabilities = calculate_class_probabilities(summaries, dataset[0])\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a prediction with Naive Bayes on Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'iris.csv'\n",
    "dataset = load_csv(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0][0] = 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iris-virginica] => 0\n",
      "[Iris-setosa] => 1\n",
      "[Iris-versicolor] => 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Iris-virginica': 0, 'Iris-setosa': 1, 'Iris-versicolor': 2}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert class column to integers\n",
    "for i in range(len(dataset[0])-1):\n",
    "    for j in range(len(dataset)):\n",
    "        dataset[j][i] = float(dataset[j][i])\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "\n",
    "#str_column_to_int(dataset, len(dataset[0])-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data=[5.7, 2.9, 4.2, 1.3], Predicted: 2\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "model = summarize_by_class(dataset)\n",
    "# define a new record\n",
    "row = [5.7,2.9,4.2,1.3]\n",
    "# predict the label\n",
    "label = predict(model, row)\n",
    "print('Data=%s, Predicted: %s' % (row, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
